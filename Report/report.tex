\documentclass[12pt]{report}

\usepackage[UKenglish]{babel}
\usepackage[margin = 1.5cm]{geometry}
\setlength{\parskip}{8pt}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{csvsimple}
\pagestyle{fancyplain}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyhfoffset[R]{0cm}
\rhead{\fancyplain{}{mk626}}
\lhead{\fancyplain{}{MichaelKubiak}}
\setlength{\headheight}{15pt}
\cfoot{\thepage}
\usepackage{xcolor}
\usepackage{natbib}
\setcitestyle{round}
\setcounter{secnumdepth}{0}
\usepackage{setspace}
\onehalfspacing
\usepackage[strings]{underscore}

\begin{document}

	\vspace*{\fill}
		\begin{center}
			\huge\textbf{Title}

			\vspace*{2cm}
			
			\large\textbf{Author:} Michael Kubiak, University of Leicester

			\vspace*{.5cm}

			\textbf{Date:} \today
			
		
		\end{center}
	\vspace*{\fill}

\pagebreak

	\section*{Abstract}
		
		
		
\pagebreak

	\tableofcontents

\pagebreak

	\section{Introduction}
		Protein annotation is a highly important stage of the analysis of organisms.  There are a number of annotation schemes that work for different sets of proteins, including Enzyme commission (EC) number, available from \cite{RefWorks:doc:5d80ae45e4b02466bec37c88}, which classifies enzymes by the reactions that they catalyse, and Gene Ontology (GO) terms (\cite{RefWorks:doc:5d80b66be4b07e3e85c92789}, \cite{RefWorks:doc:5d80b886e4b02466bec38715}), which identify and relate functions of genes and gene products (RNA and proteins) across species.  
		
		The scheme used for this initial foray into function identification by machine learning will be EC numbers, due to their relatively smaller scope (only classifying enzymes).  An EC number has 4 sections, separated by dots, each of which specifies the enzyme function to a greater degree, from general reaction type down to specific substrates. For example, as can be found in \cite{RefWorks:doc:5d70e98ce4b0ef464262611a} and its supplements, an EC number of 1.2.3.4 means that (1) the enzyme is an oxidoreductase, meaning that it catalyses oxidation/reduction, (1.2) that acts on an aldehyde or oxo group (1.2.3) with oxygen as the electron acceptor for the reaction, (1.2.3.4) and that it the specific molecule that is oxidised is an oxalate molecule.  Due to their modularity, EC numbers give levels of annotation, allowing predictions to be useful even when they are not fully accurate, or do not exactly place the specifics of the later levels.
		
		Another way that proteins are classified is by homology, providing families that evolved from a common ancestor.  Pfam \citep{RefWorks:doc:5d6e641de4b0a51fb0eed90f} holds profile 'seed' alignments, which produce hidden markov models (HMMs) for these protein families.  These can be used to recreate the profile alignment that was used to initially produce the seed alignment from the associated sequence database.  They can also be used, through programs such as HMMER \citep{RefWorks:doc:5c8f77ece4b077fbbf563f6a}, to determine the likelihood of a new protein being a member of that specific family. Proteins can have domains related to multiple families within them, and those domains may work together to provide function, making HMMER scores against the whole database better for function prediction than simply taking the best score.%TODO: More about Pfam
		
		As shown by \cite{RefWorks:doc:5d6f9c26e4b0ec3eed182252}, which describes the program ECDomainMiner, associations between EC number and Pfam family can be identified.  These associations are useful for functional annotation, since Pfam is designed to allow identification to be done against it.  Linking Pfam domains to their most likely EC numbers, in effect allows a protein to be searched against EC numbers in order to determine the chances of that protein having a relation to any one that is related to a Pfam domain.  Rather than doing this manually, so that the EC numbers related to each family are searchable, it should be possible for a machine learning algorithm to computationally relate the domains to EC numbers, and so spot patterns that may not be initially obvious, as well as increasing the speed at which possible functions can be suggested. %TODO: why is interesting
		
		Information about known proteins is stored in the "UniProt Knowledgebase" database (UniProtKB) \citep{RefWorks:doc:5d80d882e4b074875bbeabb7}.  This information includes annotations, as well as the amino acid sequences of those proteins.  UniProt itself consists of two databases, TrEMBL, which contains automatically annotated proteins that have not yet been reviewed and approved, and Swiss-Prot, which contains manually annotated and reviewed proteins.  Over time, papers manually annotating TrEMBL proteins are reviewed by a team and moved into Swiss-Prot.  This means that a piece of functional prediction software can be trained on the current Swiss-Prot database, and tested by prediction of the whole TrEMBL database, part of which prediction can be confirmed by the next release of Swiss-Prot, allowing a completely blind test. %TODO: Uni/Swissprot
		
		Machine learning allows a computer to perform a task and learn from "experience" of that task, so that it can perform better on a future attempt.  There are a number of distinct types of machine learning, including supervised, unsupervised and reinforcement learning.  The choice of general type depends on the problem that must be solved.  In a case like the one discussed in this report, supervised learning can be used.  This is the use of a large, representative dataset with known outcomes to train a model, which can then be used to predict the outcomes for data outside that initial set.  The results of each round of training are used to change the system, and bring the next set closer to the required output.  Unsupervised learning has unlabelled data, and is used for applications such as clustering and finding unknown patterns.  The final type is reinforcement learning, which is useful for interactivity.  The actions taken are provided a 'score' based on whether they bring the system closer to or further from a desired goal.  
		
		Machine learning is useful because it reduces the need for hard coded rules that a program must follow, and opens the possibility of finding patterns of relation that are as yet undiscovered.  The models built using machine learning provide methods of determining answers to questions that are difficult to answer in any other way.  
		
		One thing that increases difficulty of answering questions is the amount of data that may be related to a specific problem.  For example, the size of bioinformatics datasets has been increasing exponentially due to improved methods of obtaining data, but when particular questions are asked this abundance of data can make finding the patterns difficult.  In this particular case, the size of swissprot, along with the number of pfam families means that the relations between those and EC numbers can be difficult to discern.  Even when those relations can be found, hard coding them would be an almost impossible task, due to the time it would take.  Coding a machine learning algorithm, on the other hand, is a much shorter process.
		
		There are various libraries, in various languages, that have been written to make the process of generating a machine learning model easier.  Python, which will be used here, has a number machine learning libraries including scikit-learn \citep{RefWorks:doc:5d80f150e4b07f40b9eab2f8}, which tends to have more prewritten algorithms, and TensorFlow \citep{RefWorks:doc:5d80f20de4b08a779635c81d}, which has a toolkit that can be used to produce more specialised algorithms. % TODO: Machine learning in python
		\begin{wrapfigure}{R}{0.6\textwidth}
			\centering
			\includegraphics{random_forest.png}
			\caption{A simple illustration of a random forest, showing how each sample makes a tree that contributes to the consensus}
			\label{Figure: Random Forest}
		\end{wrapfigure}
		
		Two of the most common types of supervised machine learning are random forest classification and deep neural networks.  In random forest classification, the data is sampled (with replacement), and each sample is used to grow a decision tree.  Each node in the tree splits the data based on a particular question, for example whether an input variable is greater than a threshold value, so that the sampled data is separated back into the known classifications.  Once the forest is grown, new data is classified based on the consensus opinion of the trees in the forest.  A representation of a random forest is shown by figure \ref{Figure: Random Forest}.
		
		\begin{wrapfigure}{R}{.6\textwidth}
			\centering
			\includegraphics{neural_network.png}
			\caption{A simple illustration of a neural network with 3 hidden layers.  Weights are applied on the connections between nodes, and biases are applied on the nodes themselves, before functions are applied}
			\label{Figure: Neural Network}
		\end{wrapfigure}
		A neural network (Figure \ref{Figure: Neural Network}) consists of an input layer (the data), a set of hidden layers (the computation) and an output layer (the classification).  A deep neural network has many hidden layers.  Each layer contains a number of nodes, which are linked to every node in the subsequent layer.  The links between nodes each have a weight that determines how the output of the previous node effects the next one.  The nodes in the hidden layers each have an additive bias, which is applied to their total input, and a function that they perform, which determines their output.  The network is trained by adjusting weights and biases based on the difference between the predicted outcome and the true outcome, by a technique called backpropagation.  The use of the network is identical to the training, except without the score calculation and backpropagation.  The output layer can be comprised of continuous values, which, in a classification network, provide the probabilities that a certain classification is correct. %TODO: types of machine learning to be used?
		
		A number of functional prediction tools have been developed, including those described in \cite{RefWorks:doc:5d822c8ce4b07f40b9eae1b7}, which details the use of interaction partners of the proteins to predict function, \cite{RefWorks:doc:5d822cfce4b0506e9759e8f8}, which describes using structural 'packing patterns', and \cite{RefWorks:doc:5d822dd5e4b07f40b9eae2f4}, which uses machine learning to look at protein sequences and structural predictions.  The final is part of a further group that predict structure, rather than specifically function, and these have been judged by comptetitions such as CASP (Critical Assessment of Structure Prediction), the most recent of which was CASP13 in 2018 \citep{RefWorks:doc:5d822fa4e4b0fa10423c5184}, and its more relevant offshoot CAFASP (Critical Assessment of Fully Automated Structure Prediction).  The bodies running these competitions provide a number sequences for which they have annotations, but the groups being tested do not.  The accuracy of the predictions made is then judged, and each tool is given a score based on how it performed.  CAFASP has been superceded by another competition, CAFA (Critical Assessment of protein Function Annotation algorithms) \citep{RefWorks:doc:5d82345ae4b09beeb95d60bb} whose most recent competition CAFA took place in 2018-2019.%TODO: Other methods of function prediction
		
		Functional prediction is used as a basis for further experimentation and manual annotation, particularly of new species.  While many mammalian proteins can be identified easily based on a few homologs, many newly discovered proteins from other branches of life, do not have any close homologs that can be used to determine their function.  These require more in depth methods to be performed in order to discover their functions.  Functional prediction can be used to inform the targets of these methods, so that, even if a precise function cannot be determined, a particular set of reactions can be investigated as the most likely, based on the predictions made. %TODO: Uses for functional annotation of proteins
		
	\section{Methods}
		
		
		
	\section{Results}
		
		
				
	\section{Conclusion}
		
		
		
	\section{Future Application}
		
	
	\fancypagestyle{plain}{}
	\bibliography{export.bib}{}
	\bibliographystyle{myplainnat}

\end{document}
