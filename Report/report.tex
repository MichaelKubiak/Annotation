\documentclass[12pt]{report}

\usepackage[UKenglish]{babel}
\usepackage[margin = 1.5cm]{geometry}
\setlength{\parskip}{8pt}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{csvsimple}
\pagestyle{fancyplain}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyhfoffset[R]{0cm}
\rhead{\fancyplain{}{mk626}}
\lhead{\fancyplain{}{MichaelKubiak}}
\setlength{\headheight}{15pt}
\cfoot{\thepage}
\usepackage{xcolor}
\usepackage{natbib}
\setcitestyle{round}
\setcounter{secnumdepth}{0}
\usepackage{setspace}
\onehalfspacing
\usepackage[strings]{underscore}

\begin{document}

	\vspace*{\fill}
		\begin{center}
			\huge\textbf{Title}

			\vspace*{2cm}
			
			\large\textbf{Author:} Michael Kubiak, University of Leicester

			\vspace*{.5cm}

			\textbf{Date:} \today
			
		
		\end{center}
	\vspace*{\fill}

\pagebreak

	\section*{Abstract}
		
		
		
\pagebreak

	\tableofcontents

\pagebreak

	\section{Introduction}
		Protein annotation is a highly important stage of the analysis of organisms.  There are a number of annotation schemes that work for different sets of proteins, including Enzyme commission (EC) number, available from \cite{RefWorks:doc:5d80ae45e4b02466bec37c88}, which classifies enzymes by the reactions that they catalyse, and Gene Ontology (GO) terms (\cite{RefWorks:doc:5d80b66be4b07e3e85c92789}, \cite{RefWorks:doc:5d80b886e4b02466bec38715}), which identify and relate functions of genes and gene products (RNA and proteins) across species.  
		
		The scheme used for this initial foray into function identification by machine learning will be EC numbers, due to their relatively smaller scope (only classifying enzymes).  An EC number has 4 sections, separated by dots, each of which specifies the enzyme function to a greater degree, from general reaction type down to specific substrates. For example, as can be found in \cite{RefWorks:doc:5d70e98ce4b0ef464262611a} and its supplements, an EC number of 1.2.3.4 means that (1) the enzyme is an oxidoreductase, meaning that it catalyses oxidation/reduction, (1.2) that acts on an aldehyde or oxo group (1.2.3) with oxygen as the electron acceptor for the reaction, (1.2.3.4) and that it the specific molecule that is oxidised is an oxalate molecule.  Due to their modularity, EC numbers give levels of annotation, allowing predictions to be useful even when they are not fully accurate, or do not exactly place the specifics of the later levels.
		
		Another way that proteins are classified is by homology, providing families that evolved from a common ancestor.  Pfam \citep{RefWorks:doc:5d6e641de4b0a51fb0eed90f} holds profile 'seed' alignments, which produce hidden markov models (HMMs) for these protein families.  These can be used to recreate the profile alignment that was used to initially produce the seed alignment from the associated sequence database.  They can also be used, through programs such as HMMER \citep{RefWorks:doc:5c8f77ece4b077fbbf563f6a}, to determine the likelihood of a new protein being a member of that specific family. Proteins can have domains related to multiple families within them, and those domains may work together to provide function, making HMMER scores against the whole database better for function prediction than simply taking the best score.%More about Pfam
		
		As shown by \cite{RefWorks:doc:5d6f9c26e4b0ec3eed182252}, which describes the program ECDomainMiner, associations between EC number and Pfam family can be identified.  These associations are useful for functional annotation, since Pfam is designed to allow identification to be done against it.  Linking Pfam domains to their most likely EC numbers, in effect allows a protein to be searched against EC numbers in order to determine the chances of that protein having a relation to any one that is related to a Pfam domain.  Rather than doing this manually, so that the EC numbers related to each family are searchable, it should be possible for a machine learning algorithm to computationally relate the domains to EC numbers, and so spot patterns that may not be initially obvious, as well as increasing the speed at which possible functions can be suggested. %why is interesting
		
		Information about known proteins is stored in the "UniProt Knowledgebase" database (UniProtKB) \citep{RefWorks:doc:5d80d882e4b074875bbeabb7}.  This information includes annotations, as well as the amino acid sequences of those proteins.  UniProt itself consists of two databases, TrEMBL, which contains automatically annotated proteins that have not yet been reviewed and approved, and Swiss-Prot, which contains manually annotated and reviewed proteins.  Over time, papers manually annotating TrEMBL proteins are reviewed by a team and moved into Swiss-Prot.  This means that a piece of functional prediction software can be trained on the current Swiss-Prot database, and tested by prediction of the whole TrEMBL database, part of which prediction can be confirmed by the next release of Swiss-Prot, allowing a completely blind test. %Uni/Swissprot
		
		Machine learning allows a computer to perform a task and learn from "experience" of that task, so that it can perform better on a future attempt.  There are a number of distinct types of machine learning, including supervised, unsupervised and reinforcement learning.  The choice of general type depends on the problem that must be solved, as well as that data that is available.  In a case like the one discussed in this report, supervised learning can be used.  This is the use of a large, representative dataset with known outcomes to train a model, which can then be used to predict the outcomes for data outside that initial set.  Training of a supervised machine learning algorithm is relatively simple, as there is a required output, so the output can be tested for how accurate it is.  Unsupervised learning has unlabelled data, and is used for applications such as clustering and finding unknown patterns.  Knowing whether an unsupervised algorithm has produced an answer that is related to the question being asked is difficult due to the difficulty of knowing how the decisions were made.  The final type is reinforcement learning is somewhat similar to supervised learning, as there is direction, based on a required outcome (winning a game of chess, etc.).  The actions taken are provided a 'score' based on whether they bring the system closer to or further from a desired goal.  This makes it useful in cases where interaction with an outside agent is required.
		
		Machine learning is useful because it reduces the need for hard coded rules that a program must follow, and opens the possibility of finding patterns of relation that are as yet undiscovered.  The models built using machine learning provide methods of determining answers to questions that are difficult to answer in any other way.  
		
		One thing that increases difficulty of answering questions is the amount of data that may be related to a specific problem.  For example, the size of bioinformatics datasets has been increasing exponentially due to improved methods of obtaining data, but when particular questions are asked this abundance of data can make finding the patterns difficult.  In this particular case, the size of swissprot, along with the number of pfam families means that the relations between those and EC numbers can be difficult to discern.  Even when those relations can be found, hard coding them would be an almost impossible task, due to the time it would take.  Coding a machine learning algorithm, on the other hand, is a much shorter process.
		
		There are various libraries, in various languages, that have been written to make the process of generating a machine learning model easier.  Python, which will be used here, has a number machine learning libraries.  One of these is Scikit-learn \citep{RefWorks:doc:5d80f150e4b07f40b9eab2f8}, which has a large number of prewritten algorithms for many different methods, including various classification, regression and clustering algorithms, as well as data preparation modules for uses such as dimensionality reduction.  Another option, TensorFlow \citep{RefWorks:doc:5d80f20de4b08a779635c81d} has a toolkit that can be used to produce more specialised algorithms, so that the machine learning solution can be tailored to the task. % TODO: Machine learning in python
		
		Three of the most common types of supervised machine learning are random forest classification, initially suggested in \cite{RefWorks:doc:5d84b84de4b03ee47d60013e}, and first used in \cite{RefWorks:doc:5d84b954e4b01cdccc094821}, support vector machines, whose first appearance in literature is in \cite{RefWorks:doc:5d84bca0e4b074abc390dc95} and deep neural networks, which is an extension of the idea of neural networks, the mathematics of which were first proposed in \cite{RefWorks:doc:5d84d2d4e4b048bf85a1aa0a}.  

		\begin{wrapfigure}{L}{0.6\textwidth}
			\centering
			\includegraphics{random_forest.png}
			\caption{A simple illustration of a random forest, showing how each of a number (n) of samples makes a tree that contributes to the consensus that will be reported}
			\label{Figure: Random Forest}
		\end{wrapfigure}
		
		In random forest classification, the data is sampled (with replacement), and each sample is used to grow a decision tree.  Each node in the tree splits the data based on a particular question, for example whether an input variable is greater than a threshold value, so that the sampled data is separated back into the known classifications.  Once the forest is grown, new data is classified based on the consensus opinion of the trees in the forest.  A representation of a random forest is shown by figure \ref{Figure: Random Forest}.  Random forests have very few parameters that can be tuned to change their performance, i.e. sample size and tree number/depth.  They have multiclass classification built in due to the nature of decision trees, where each branch can lead to a certain classification.  This means that they make a good initial benchmark for any machine learning project.
		
		A Support vector machine (SVM) attempts to maximise the 'distance' between the decision boundary, the plane (in a plot of the data points) by which classifications are separated, and the points that it separates.  SVMs are less likely to be specific to a training dataset because the boundaries are not being designed only to separate the classes, in which case they might curve around a particular point that is somewhat of an outlier to a group, but instead to ensure that there is the greatest margin between the decision boundary and the points that is is separating.  In that case of non linearly separable problems, SVMs use the 'kernal trick' to transform the data to a higher dimensional space, in which it becomes linearly separable.  The choice of kernal (transformation function), as well as how strict the SVM is about incorrect classification are variable, and can be used to tune for better performance.  SVMs usually rely upon One vs All (OvA) classification, meaning that they recognise whether a sample is each possible classification in turn, rather than determining all memberships at the same time.
		
		\begin{wrapfigure}{R}{.6\textwidth}
			\centering
			\includegraphics{neural_network.png}
			\caption{A simple illustration of a neural network with 3 hidden layers.  Weights are applied on the connections between nodes, and biases are applied on the nodes themselves, before functions are applied}
			\label{Figure: Neural Network}
		\end{wrapfigure}
		
		A neural network (Figure \ref{Figure: Neural Network}) consists of an input layer (the data), a set of hidden layers (the computation) and an output layer (the classification).  A deep neural network has many hidden layers, which allows it to model more complex systems.  Each layer contains a number of nodes, which are linked to every node in the subsequent layer.  The links between nodes each have a weight that determines how the output of the previous node effects the next one.  The nodes in the hidden layers each have a bias, which is applied to their total input, and a function that they perform, which determines their output.  The network is trained by putting a training value, whose expected output is known,  adjusting weights and biases based on the difference between the predicted outcome and the true outcome, by a technique called backpropagation.  The use of the network is identical to the training, except without the score calculation and backpropagation.  The output layer can be comprised of continuous values, which, in a classification network, provide the probabilities that a certain classification is correct.  As with the SVM, a deep neural network is highly tunable, with factors such as number of hidden layers, functions on the nodes, and even learning rate of the system (how much weights and biases are altered during each backpropagation step.  %types of machine learning to be used
		
		A number of functional prediction tools have been developed.  One such approach is that described in \cite{RefWorks:doc:5d822c8ce4b07f40b9eae1b7}, which details the use of sequence similarity, as well as interaction partners of the proteins to predict its action as an enzyme.  This approach requires a good deal of experimental observation, as well as computational methods for each protein.  It builds upon previous attempts at prediction, which used only the interaction partners, and no computational methods at all.  A second example of protein function prediction is \cite{RefWorks:doc:5d822cfce4b0506e9759e8f8}, which describes using structural 'packing patterns' of amino acids that are common in specific families, but much rarer in PDB in general.  Again, this method requires experimental work, and, specifically, it requires that a structure be produced, so that the packing of amino acids can be identified. 
		
		\cite{RefWorks:doc:5d88a6d8e4b08db974488b16} describes the use of three classification techinques, SVM, Sequential Minimal Optimisation \citep{RefWorks:doc:5d88bafce4b0d12609fd641e}, which is an extension of SVM which improves learning time, and Adaptive Boosting (\cite{RefWorks:doc:5d88b8c6e4b0732b5fdb9dd5} and \cite{RefWorks:doc:5d88b89ce4b0d12609fd639c}), which combines the output of other machine learning techniques to produce a weighted sum, to relate families and groups found by InterPro \citep{RefWorks:doc:5d88a822e4b0d12609fd5fc4} to GO terms.  This method is much more similar to the intentions of this project, since it is entirely computational in nature, and uses calculated families to determine likely function.
		
		The ``DeepMind'' project, funded by Google, has attempted structural prediction, rather than functional prediction, through its offshoot "AlphaFold", described in \cite{RefWorks:doc:5d89ec21e4b02d8374a7bbe7}, which used deep neural networks to predict protein folding. \cite{RefWorks:doc:5d822dd5e4b07f40b9eae2f4} also performs structural prediction, these could be thought of as less closely related to this project, but the second example, above, shows that structural information can also be used in functional prediction.  These two functional predictors have been used as part of a group that are judged by comptetitions such as CASP (Critical Assessment of Structure Prediction), the most recent of which was CASP13 in 2018 \citep{RefWorks:doc:5d822fa4e4b0fa10423c5184}.  CASP provides teams with a set of proteins whose structures are unknown, but will be worked on experimentally once models are submitted.  Each team produces predictions based on their modelling software, and those models are graded based on their accuracy.  Its offshoot, CAFASP (Critical Assessment of Fully Automated Structure Prediction) was a similar system, except that no human input was allowed, the modelling software was uploaded to a server, and its raw output was graded.  
		
		CAFASP has been superceded by another competition, CAFA (Critical Assessment of protein Function Annotation) \citep{RefWorks:doc:5d82345ae4b09beeb95d60bb} whose most recent competition CAFA took place in 2018-2019.  CAFA is more relevant to this project, since the algorithms perform annotation, rather than structure prediction.  %TODO: Other methods of function prediction
		
		The purpose of all of these competitions is to assess how far the field of computational prediction has come in their areas, so that further advances can be made using techniques that iterate on the most successful.
		
		Functional annotation prediction is used as a basis for further experimentation and manual annotation, particularly of new species.  While many mammalian proteins can be identified easily based on a few homologs in closely related species, mammals are only a small section of the life that exists on the planet.  Even proteins that are easily annotated to a primary function may have other, less obvious, functions that might be ignored unless discovered by functional prediction.  Many newly discovered proteins from other branches of life do not have any close homologs that can be used to determine their function, and so are much more difficult to annotate.  These require more in-depth, time consuming methods to be performed in order to discover their functions, and so anything that reduces the time taken by those methods is very useful.  Functional prediction can be used to inform the starting points and possiblilities explored through these methods, so that, in the case of enzymes, even if a precise function cannot be determined, a particular set of reactions can be investigated as the most likely, based on the predictions made.  %TODO: Uses for functional annotation of proteins 
		
		The project will attempt to implement some of the functionality of \cite{RefWorks:doc:5d88bf0fe4b037ddf3555c0b}, but using machine learning, and allowing for multidomain proteins.  The intention is to attempt to use supervised machine learning to produce an alogrithm which annotates a protein based on its HMMER scores against the Pfam HMMs.  The learning will be done using proteins from the Swissprot database.  Since the enzymes among these have EC numbers assigned to them, those classifications can be used as targets for the algorithm to aim towards.  It is expected that a number of different supervised machine learning algorithms will be tried, starting with a random forest, and developing from there, based on success or failure of that approach, and any advice that more experienced machine learning users might provide.  The variety of attempts will allow the best implementation to be discovered.  %TODO: My plans
		
	\section{Methods}
		
		
		
	\section{Results}
		
		
				
	\section{Conclusion}
		
		
		
	\section{Future Application}
		
	
	\fancypagestyle{plain}{}
	\bibliography{export.bib}{}
	\bibliographystyle{myplainnat}

\end{document}
